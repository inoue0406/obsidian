Although transformer based architectures have been applied successfully in the field of Language modeling and image recognition, understanding spatiotemporal series forecasting remains very difficult because of high dimensionality.

Transformer-based model should be able to interpolate or extrapolate at any scale.

The use of equally gridded data has been popular, but this is curse rather than blessing when understanding meteorological phenomena. 

Considering these topics we propose a transformer based machine learning setting where we exploit positional encoding to allow much more flexible modeling.

“Transformer is the most generic thing that we can train on” (Yanick Klicher)

Although the transformer model itself has fixed input and output, we elaborate a stochastic pre-process and post-process to enable high-dimensional input/output.

Reference:
[[Formalization of Transformer-based meteorological interpolator (GeoTrans)]]
